{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import HappinessDataset\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ctgan import CTGAN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pgmpy\n",
    "!pip install copulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(name)s:%(lineno)s [%(levelname)s]: %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src.dataset:73 [INFO]: Loaded data for years: ['2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n"
     ]
    }
   ],
   "source": [
    "DATA = HappinessDataset.from_kaggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ twenty_xy=2015 and xy=15 have the same df. :)\n",
      "✅ twenty_xy=2016 and xy=16 have the same df. :)\n",
      "✅ twenty_xy=2017 and xy=17 have the same df. :)\n",
      "✅ twenty_xy=2018 and xy=18 have the same df. :)\n",
      "✅ twenty_xy=2019 and xy=19 have the same df. :)\n",
      "✅ twenty_xy=2020 and xy=20 have the same df. :)\n",
      "✅ twenty_xy=2021 and xy=21 have the same df. :)\n",
      "✅ twenty_xy=2022 and xy=22 have the same df. :)\n",
      "✅ twenty_xy=2023 and xy=23 have the same df. :)\n"
     ]
    }
   ],
   "source": [
    "CHECK_MARK = \"\\u2705\"\n",
    "CROSS_MARK = \"\\u274C\"\n",
    "\n",
    "for twenty_xy_str in DATA.get_years():\n",
    "    twenty_xy = int(twenty_xy_str)\n",
    "    xy = twenty_xy - 2000\n",
    "\n",
    "    assert (\n",
    "        DATA[twenty_xy] is DATA[xy]\n",
    "    ), f\"{CROSS_MARK} failed at year {twenty_xy}\"\n",
    "\n",
    "    print(f\"{CHECK_MARK} {twenty_xy=} and {xy=} have the same df. :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2023 = DATA[2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2023.to_csv(\"happiness_2023.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding noice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_2023\n",
    "\n",
    "def add_noise(df, noise_level=0.05):\n",
    "    noisy_df = df.copy()\n",
    "    numeric_cols = [\n",
    "        'happiness_score', 'gdp_per_capita', 'social_support',\n",
    "        'healthy_life_expectancy', 'freedom_to_make_life_choices',\n",
    "        'generosity', 'perceptions_of_corruption'\n",
    "    ]\n",
    "    for col in numeric_cols:\n",
    "        noise = np.random.normal(0, noise_level * df[col].std(), size=df[col].shape)\n",
    "        noisy_df[col] += noise\n",
    "    return noisy_df\n",
    "\n",
    "# Generate multiple synthetic datasets\n",
    "num_synthetic_copies = 5  # Adjust this number to generate more or fewer datasets\n",
    "synthetic_dfs = [add_noise(df, noise_level=0.05) for _ in range(num_synthetic_copies)]\n",
    "\n",
    "# Concatenate the original DataFrame with all synthetic DataFrames\n",
    "final_df = pd.concat([df] + synthetic_dfs, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>happiness_score</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>social_support</th>\n",
       "      <th>healthy_life_expectancy</th>\n",
       "      <th>freedom_to_make_life_choices</th>\n",
       "      <th>generosity</th>\n",
       "      <th>perceptions_of_corruption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>822.000000</td>\n",
       "      <td>822.000000</td>\n",
       "      <td>822.000000</td>\n",
       "      <td>816.000000</td>\n",
       "      <td>822.000000</td>\n",
       "      <td>822.000000</td>\n",
       "      <td>822.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.539679</td>\n",
       "      <td>1.407774</td>\n",
       "      <td>1.155933</td>\n",
       "      <td>0.365935</td>\n",
       "      <td>0.539989</td>\n",
       "      <td>0.148579</td>\n",
       "      <td>0.145864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.136720</td>\n",
       "      <td>0.431698</td>\n",
       "      <td>0.325811</td>\n",
       "      <td>0.156362</td>\n",
       "      <td>0.149031</td>\n",
       "      <td>0.075828</td>\n",
       "      <td>0.126423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.743263</td>\n",
       "      <td>-0.025029</td>\n",
       "      <td>-0.018121</td>\n",
       "      <td>-0.012594</td>\n",
       "      <td>-0.012086</td>\n",
       "      <td>-0.002885</td>\n",
       "      <td>-0.007882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.728543</td>\n",
       "      <td>1.094050</td>\n",
       "      <td>0.966122</td>\n",
       "      <td>0.248234</td>\n",
       "      <td>0.455905</td>\n",
       "      <td>0.097558</td>\n",
       "      <td>0.058733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.684448</td>\n",
       "      <td>1.446456</td>\n",
       "      <td>1.231278</td>\n",
       "      <td>0.393053</td>\n",
       "      <td>0.557307</td>\n",
       "      <td>0.136290</td>\n",
       "      <td>0.109753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.354949</td>\n",
       "      <td>1.787836</td>\n",
       "      <td>1.402117</td>\n",
       "      <td>0.486830</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>0.198089</td>\n",
       "      <td>0.186260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.875061</td>\n",
       "      <td>2.215017</td>\n",
       "      <td>1.637111</td>\n",
       "      <td>0.711225</td>\n",
       "      <td>0.777815</td>\n",
       "      <td>0.430263</td>\n",
       "      <td>0.570001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       happiness_score  gdp_per_capita  social_support  \\\n",
       "count       822.000000      822.000000      822.000000   \n",
       "mean          5.539679        1.407774        1.155933   \n",
       "std           1.136720        0.431698        0.325811   \n",
       "min           1.743263       -0.025029       -0.018121   \n",
       "25%           4.728543        1.094050        0.966122   \n",
       "50%           5.684448        1.446456        1.231278   \n",
       "75%           6.354949        1.787836        1.402117   \n",
       "max           7.875061        2.215017        1.637111   \n",
       "\n",
       "       healthy_life_expectancy  freedom_to_make_life_choices  generosity  \\\n",
       "count               816.000000                    822.000000  822.000000   \n",
       "mean                  0.365935                      0.539989    0.148579   \n",
       "std                   0.156362                      0.149031    0.075828   \n",
       "min                  -0.012594                     -0.012086   -0.002885   \n",
       "25%                   0.248234                      0.455905    0.097558   \n",
       "50%                   0.393053                      0.557307    0.136290   \n",
       "75%                   0.486830                      0.656000    0.198089   \n",
       "max                   0.711225                      0.777815    0.430263   \n",
       "\n",
       "       perceptions_of_corruption  \n",
       "count                 822.000000  \n",
       "mean                    0.145864  \n",
       "std                     0.126423  \n",
       "min                    -0.007882  \n",
       "25%                     0.058733  \n",
       "50%                     0.109753  \n",
       "75%                     0.186260  \n",
       "max                     0.570001  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>happiness_score</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>social_support</th>\n",
       "      <th>healthy_life_expectancy</th>\n",
       "      <th>freedom_to_make_life_choices</th>\n",
       "      <th>generosity</th>\n",
       "      <th>perceptions_of_corruption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.539796</td>\n",
       "      <td>1.406985</td>\n",
       "      <td>1.156212</td>\n",
       "      <td>0.366176</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.148474</td>\n",
       "      <td>0.145898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.139929</td>\n",
       "      <td>0.432963</td>\n",
       "      <td>0.326322</td>\n",
       "      <td>0.156691</td>\n",
       "      <td>0.149501</td>\n",
       "      <td>0.076053</td>\n",
       "      <td>0.126723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.859000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.724000</td>\n",
       "      <td>1.099000</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>0.248500</td>\n",
       "      <td>0.455000</td>\n",
       "      <td>0.097000</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.684000</td>\n",
       "      <td>1.449000</td>\n",
       "      <td>1.227000</td>\n",
       "      <td>0.389500</td>\n",
       "      <td>0.557000</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.334000</td>\n",
       "      <td>1.798000</td>\n",
       "      <td>1.401000</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>0.199000</td>\n",
       "      <td>0.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.804000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1.620000</td>\n",
       "      <td>0.702000</td>\n",
       "      <td>0.772000</td>\n",
       "      <td>0.422000</td>\n",
       "      <td>0.561000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       happiness_score  gdp_per_capita  social_support  \\\n",
       "count       137.000000      137.000000      137.000000   \n",
       "mean          5.539796        1.406985        1.156212   \n",
       "std           1.139929        0.432963        0.326322   \n",
       "min           1.859000        0.000000        0.000000   \n",
       "25%           4.724000        1.099000        0.962000   \n",
       "50%           5.684000        1.449000        1.227000   \n",
       "75%           6.334000        1.798000        1.401000   \n",
       "max           7.804000        2.200000        1.620000   \n",
       "\n",
       "       healthy_life_expectancy  freedom_to_make_life_choices  generosity  \\\n",
       "count               136.000000                    137.000000  137.000000   \n",
       "mean                  0.366176                      0.540000    0.148474   \n",
       "std                   0.156691                      0.149501    0.076053   \n",
       "min                   0.000000                      0.000000    0.000000   \n",
       "25%                   0.248500                      0.455000    0.097000   \n",
       "50%                   0.389500                      0.557000    0.137000   \n",
       "75%                   0.487500                      0.656000    0.199000   \n",
       "max                   0.702000                      0.772000    0.422000   \n",
       "\n",
       "       perceptions_of_corruption  \n",
       "count                 137.000000  \n",
       "mean                    0.145898  \n",
       "std                     0.126723  \n",
       "min                     0.000000  \n",
       "25%                     0.060000  \n",
       "50%                     0.111000  \n",
       "75%                     0.187000  \n",
       "max                     0.561000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rdt.transformers.null:119 [INFO]: Guidance: There are no missing values in column happiness_score. Extra column not created.\n",
      "rdt.transformers.null:119 [INFO]: Guidance: There are no missing values in column gdp_per_capita. Extra column not created.\n",
      "rdt.transformers.null:119 [INFO]: Guidance: There are no missing values in column social_support. Extra column not created.\n",
      "rdt.transformers.null:119 [INFO]: Guidance: There are no missing values in column freedom_to_make_life_choices. Extra column not created.\n",
      "rdt.transformers.null:119 [INFO]: Guidance: There are no missing values in column generosity. Extra column not created.\n",
      "rdt.transformers.null:119 [INFO]: Guidance: There are no missing values in column perceptions_of_corruption. Extra column not created.\n"
     ]
    }
   ],
   "source": [
    "from ctgan import CTGAN\n",
    "from ctgan import load_demo\n",
    "\n",
    "synth_data = df.drop(columns=[\"country\",  'healthy_life_expectancy'])\n",
    "\n",
    "discrete_columns = ['region']\n",
    "\n",
    "ctgan = CTGAN(epochs=10)\n",
    "ctgan.fit(synth_data, discrete_columns)\n",
    "\n",
    "# Create synthetic data\n",
    "synthetic_data = ctgan.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>happiness_score</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>social_support</th>\n",
       "      <th>freedom_to_make_life_choices</th>\n",
       "      <th>generosity</th>\n",
       "      <th>perceptions_of_corruption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.244884</td>\n",
       "      <td>2.039567</td>\n",
       "      <td>0.792013</td>\n",
       "      <td>0.578550</td>\n",
       "      <td>0.173524</td>\n",
       "      <td>0.239614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.465174</td>\n",
       "      <td>0.387813</td>\n",
       "      <td>0.457518</td>\n",
       "      <td>0.219878</td>\n",
       "      <td>0.124831</td>\n",
       "      <td>0.181772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.789742</td>\n",
       "      <td>0.095347</td>\n",
       "      <td>-0.249484</td>\n",
       "      <td>-0.242032</td>\n",
       "      <td>-0.173418</td>\n",
       "      <td>-0.115263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.198996</td>\n",
       "      <td>1.840935</td>\n",
       "      <td>0.397811</td>\n",
       "      <td>0.446626</td>\n",
       "      <td>0.084620</td>\n",
       "      <td>0.101081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.178791</td>\n",
       "      <td>2.104570</td>\n",
       "      <td>0.922344</td>\n",
       "      <td>0.603360</td>\n",
       "      <td>0.166471</td>\n",
       "      <td>0.192688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.253108</td>\n",
       "      <td>2.324664</td>\n",
       "      <td>1.140549</td>\n",
       "      <td>0.747861</td>\n",
       "      <td>0.250151</td>\n",
       "      <td>0.372063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.381159</td>\n",
       "      <td>2.804300</td>\n",
       "      <td>1.735828</td>\n",
       "      <td>0.966621</td>\n",
       "      <td>0.598372</td>\n",
       "      <td>0.757771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       happiness_score  gdp_per_capita  social_support  \\\n",
       "count      1000.000000     1000.000000     1000.000000   \n",
       "mean          4.244884        2.039567        0.792013   \n",
       "std           1.465174        0.387813        0.457518   \n",
       "min           0.789742        0.095347       -0.249484   \n",
       "25%           3.198996        1.840935        0.397811   \n",
       "50%           4.178791        2.104570        0.922344   \n",
       "75%           5.253108        2.324664        1.140549   \n",
       "max           8.381159        2.804300        1.735828   \n",
       "\n",
       "       freedom_to_make_life_choices   generosity  perceptions_of_corruption  \n",
       "count                   1000.000000  1000.000000                1000.000000  \n",
       "mean                       0.578550     0.173524                   0.239614  \n",
       "std                        0.219878     0.124831                   0.181772  \n",
       "min                       -0.242032    -0.173418                  -0.115263  \n",
       "25%                        0.446626     0.084620                   0.101081  \n",
       "50%                        0.603360     0.166471                   0.192688  \n",
       "75%                        0.747861     0.250151                   0.372063  \n",
       "max                        0.966621     0.598372                   0.757771  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "       country                        region  happiness_score  gdp_per_capita  \\\n",
      "0      Finland                Western Europe            7.804           1.888   \n",
      "1      Denmark                Western Europe            7.586           1.949   \n",
      "2      Iceland                Western Europe            7.530           1.926   \n",
      "3       Israel  Middle East and North Africa            7.473           1.833   \n",
      "4  Netherlands                Western Europe            7.403           1.942   \n",
      "\n",
      "   social_support  healthy_life_expectancy  freedom_to_make_life_choices  \\\n",
      "0           1.585                    0.535                         0.772   \n",
      "1           1.548                    0.537                         0.734   \n",
      "2           1.620                    0.559                         0.738   \n",
      "3           1.521                    0.577                         0.569   \n",
      "4           1.488                    0.545                         0.672   \n",
      "\n",
      "   generosity  perceptions_of_corruption  \n",
      "0       0.126                      0.535  \n",
      "1       0.208                      0.525  \n",
      "2       0.250                      0.187  \n",
      "3       0.124                      0.158  \n",
      "4       0.251                      0.394  \n",
      "\n",
      "Missing Values Before Imputation:\n",
      "region                          0\n",
      "happiness_score                 0\n",
      "gdp_per_capita                  0\n",
      "social_support                  0\n",
      "healthy_life_expectancy         1\n",
      "freedom_to_make_life_choices    0\n",
      "generosity                      0\n",
      "perceptions_of_corruption       0\n",
      "dtype: int64\n",
      "\n",
      "Missing Values After Imputation:\n",
      "region                          0\n",
      "happiness_score                 0\n",
      "gdp_per_capita                  0\n",
      "social_support                  0\n",
      "healthy_life_expectancy         0\n",
      "freedom_to_make_life_choices    0\n",
      "generosity                      0\n",
      "perceptions_of_corruption       0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/st/q8k3m57x71bbmhhvc1l_t3vh0000gn/T/ipykernel_41956/2276236062.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_synth['region'].fillna(df_synth['region'].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Drop unique identifier 'country'\n",
    "df_synth = df.drop(['country'], axis=1)\n",
    "\n",
    "# Encode categorical variables (e.g., 'region')\n",
    "df_synth['region'] = df_synth['region'].astype('category').cat.codes\n",
    "\n",
    "# Reset index\n",
    "df_synth = df_synth.reset_index(drop=True)\n",
    "\n",
    "# Handle missing values if any\n",
    "print(\"\\nMissing Values Before Imputation:\")\n",
    "print(df_synth.isnull().sum())\n",
    "\n",
    "# Fill numerical missing values with mean\n",
    "numeric_cols = [\n",
    "    'happiness_score', 'gdp_per_capita', 'social_support',\n",
    "    'healthy_life_expectancy', 'freedom_to_make_life_choices',\n",
    "    'generosity', 'perceptions_of_corruption'\n",
    "]\n",
    "df_synth[numeric_cols] = df_synth[numeric_cols].fillna(df_synth[numeric_cols].mean())\n",
    "\n",
    "# Fill categorical missing values with mode\n",
    "df_synth['region'].fillna(df_synth['region'].mode()[0], inplace=True)\n",
    "\n",
    "print(\"\\nMissing Values After Imputation:\")\n",
    "print(df_synth.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "File \u001b[0;32m~/Desktop/AUTO/acim-happiness/.venv/lib/python3.12/site-packages/tensorflow/__init__.py:30\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03mTop-level module of TensorFlow. By convention, we refer to this module as\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m`tf` instead of `tensorflow`, following the common practice of importing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03mthis file with a file generated from [`api_template.__init__.py`](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/api_template.__init__.py)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-bad-import-order,protected-access,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_distutils\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_inspect\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'distutils'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def generate_synthetic_data_vae(df, num_samples=400, latent_dim=2):\n",
    "    \"\"\"Generates synthetic data using a Variational Autoencoder (VAE).\n",
    "\n",
    "    Args:\n",
    "        df: The original pandas DataFrame containing the data.\n",
    "        num_samples: The number of synthetic samples to generate.\n",
    "        latent_dim: The dimensionality of the latent space.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the synthetic data.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Data Preprocessing\n",
    "    # a. Handle NaN/inf values\n",
    "    df.fillna(df.mean(), inplace=True)  # Impute NaNs with mean\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna()  # Remove infinite values\n",
    "\n",
    "    # b. Scale numerical features\n",
    "    scaler = MinMaxScaler()\n",
    "    numerical_cols = ['happiness_score', 'gdp_per_capita', 'social_support',\n",
    "                       'healthy_life_expectancy', 'freedom_to_make_life_choices',\n",
    "                       'generosity', 'perceptions_of_corruption']\n",
    "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "    # 2. Build the VAE Model\n",
    "    class Sampling(layers.Layer):\n",
    "        \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var = inputs\n",
    "            batch = tf.shape(z_mean)[0]\n",
    "            dim = tf.shape(z_mean)[1]\n",
    "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    # Encoder\n",
    "    encoder_inputs = keras.Input(shape=(df.shape[1],))\n",
    "    x = layers.Dense(64, activation=\"relu\")(encoder_inputs)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "    # Decoder\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(32, activation=\"relu\")(latent_inputs)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    decoder_outputs = layers.Dense(df.shape[1], activation=\"sigmoid\")(x)  # Sigmoid for scaled data\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "    # VAE\n",
    "    outputs = decoder(encoder(encoder_inputs)[2])\n",
    "    vae = keras.Model(encoder_inputs, outputs, name=\"vae\")\n",
    "\n",
    "    # 3. Train the VAE\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)  # Adjusted learning rate\n",
    "    vae.compile(optimizer=optimizer, loss=\"mse\") \n",
    "    vae.fit(df, df, epochs=100, batch_size=32)  # Adjusted hyperparameters\n",
    "\n",
    "    # 4. Generate Synthetic Data\n",
    "    random_latent_vectors = tf.random.normal(shape=(num_samples, latent_dim))\n",
    "    synthetic_data = decoder.predict(random_latent_vectors)\n",
    "\n",
    "    # 5. Inverse Transform (Scaling)\n",
    "    synthetic_data = scaler.inverse_transform(synthetic_data)  # Rescale to original range\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)\n",
    "\n",
    "    return synthetic_df\n",
    "\n",
    "no_countries_df = df.drop(['country', 'region'], axis=1)\n",
    "\n",
    "synthetic_df = generate_synthetic_data_vae(no_countries_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copulas.multivariate import GaussianMultivariate\n",
    "# Assuming your original data is in a DataFrame called 'df'\n",
    "\n",
    "def generate_synthetic_data_copula(df, num_samples=137):\n",
    "    \"\"\"Generates synthetic data using a Gaussian copula.\n",
    "\n",
    "    Args:\n",
    "        df: The original pandas DataFrame containing the data.\n",
    "        num_samples: The number of synthetic samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the synthetic data.\n",
    "    \"\"\"\n",
    "    df.fillna(df.mean(), inplace=True)  # Impute NaNs with mean\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna()  # Remove infinite values\n",
    "\n",
    "    # b. Scale numerical features\n",
    "    scaler = MinMaxScaler()\n",
    "    numerical_cols = ['happiness_score', 'gdp_per_capita', 'social_support',\n",
    "                       'healthy_life_expectancy', 'freedom_to_make_life_choices',\n",
    "                       'generosity', 'perceptions_of_corruption']\n",
    "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "    # 1. Fit a Gaussian copula to the data\n",
    "    copula = GaussianMultivariate()\n",
    "    copula.fit(df)\n",
    "\n",
    "    # 2. Generate synthetic data from the copula\n",
    "    synthetic_data = copula.sample(num_samples)\n",
    "\n",
    "    synthetic_data = scaler.inverse_transform(synthetic_data)  # Rescale to original range\n",
    "\n",
    "    # 3. Convert to DataFrame\n",
    "    synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)\n",
    "    \n",
    "\n",
    "    return synthetic_df\n",
    "\n",
    "no_countries_df = df.drop(['country', 'region'], axis=1)\n",
    "synthetic_df = generate_synthetic_data_copula(no_countries_df)\n",
    "print(synthetic_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
